{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "79894cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87e64a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0],\n",
    "              [0, 1], \n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1], \n",
    "              [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de3eb851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "320c2df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "983c18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dC_da(a, y_true):\n",
    "    return 2*(a- y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "10ea45e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def da_dz(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0feaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size: int, neuron_count: int):\n",
    "        self.W = np.random.randn(input_size, neuron_count)\n",
    "        self.b = np.zeros((1, neuron_count))\n",
    "        \n",
    "        self.z = None\n",
    "        self.inputs = None\n",
    "        self.activations = None\n",
    "        \n",
    "    def forward(self, X: np.ndarray):   \n",
    "        self.inputs = X\n",
    "        self.z = X @ self.W + self.b\n",
    "        a = sigmoid(self.z)\n",
    "        self.activations = a\n",
    "        return a\n",
    "    \n",
    "    def backward(self, delta, learning_rate):\n",
    "        dz = np.array([])\n",
    "        \n",
    "\n",
    "        dz = delta * da_dz(self.z)\n",
    "        \n",
    "        dW = self.inputs.T @ dz\n",
    "        db = np.sum(dz, axis=0, keepdims=True)\n",
    "        \n",
    "        delta_prev = dz @ self.W.T\n",
    "        \n",
    "        self.W -= learning_rate * dW\n",
    "        self.b -= learning_rate * db\n",
    "        \n",
    "        return delta_prev\n",
    "    \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2049768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       , 0.5       ],\n",
       "       [0.47854632, 0.4531206 , 0.47137274],\n",
       "       [0.67499085, 0.52453484, 0.27968064],\n",
       "       [0.65587796, 0.47755216, 0.2571794 ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = Layer(input_size=2, neuron_count=3)\n",
    "layer.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ac5a70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.254471\n",
      "Epoch 100, Loss: 0.242544\n",
      "Epoch 200, Loss: 0.194102\n",
      "Epoch 300, Loss: 0.089168\n",
      "Epoch 400, Loss: 0.027305\n",
      "Epoch 500, Loss: 0.015566\n",
      "Epoch 600, Loss: 0.011082\n",
      "Epoch 700, Loss: 0.008718\n",
      "Epoch 800, Loss: 0.007250\n",
      "Epoch 900, Loss: 0.006246\n",
      "Epoch 1000, Loss: 0.005512\n",
      "Epoch 1100, Loss: 0.004950\n",
      "Epoch 1200, Loss: 0.004505\n",
      "Epoch 1300, Loss: 0.004143\n",
      "Epoch 1400, Loss: 0.003843\n",
      "Epoch 1500, Loss: 0.003588\n",
      "Epoch 1600, Loss: 0.003370\n",
      "Epoch 1700, Loss: 0.003181\n",
      "Epoch 1800, Loss: 0.003015\n",
      "Epoch 1900, Loss: 0.002868\n",
      "Epoch 2000, Loss: 0.002737\n",
      "Epoch 2100, Loss: 0.002619\n",
      "Epoch 2200, Loss: 0.002513\n",
      "Epoch 2300, Loss: 0.002416\n",
      "Epoch 2400, Loss: 0.002328\n",
      "Epoch 2500, Loss: 0.002246\n",
      "Epoch 2600, Loss: 0.002172\n",
      "Epoch 2700, Loss: 0.002103\n",
      "Epoch 2800, Loss: 0.002039\n",
      "Epoch 2900, Loss: 0.001979\n",
      "Epoch 3000, Loss: 0.001923\n",
      "Epoch 3100, Loss: 0.001871\n",
      "Epoch 3200, Loss: 0.001823\n",
      "Epoch 3300, Loss: 0.001777\n",
      "Epoch 3400, Loss: 0.001734\n",
      "Epoch 3500, Loss: 0.001693\n",
      "Epoch 3600, Loss: 0.001654\n",
      "Epoch 3700, Loss: 0.001618\n",
      "Epoch 3800, Loss: 0.001583\n",
      "Epoch 3900, Loss: 0.001550\n",
      "Epoch 4000, Loss: 0.001519\n",
      "Epoch 4100, Loss: 0.001489\n",
      "Epoch 4200, Loss: 0.001460\n",
      "Epoch 4300, Loss: 0.001433\n",
      "Epoch 4400, Loss: 0.001407\n",
      "Epoch 4500, Loss: 0.001382\n",
      "Epoch 4600, Loss: 0.001358\n",
      "Epoch 4700, Loss: 0.001335\n",
      "Epoch 4800, Loss: 0.001313\n",
      "Epoch 4900, Loss: 0.001292\n",
      "Epoch 5000, Loss: 0.001272\n",
      "Epoch 5100, Loss: 0.001252\n",
      "Epoch 5200, Loss: 0.001233\n",
      "Epoch 5300, Loss: 0.001215\n",
      "Epoch 5400, Loss: 0.001198\n",
      "Epoch 5500, Loss: 0.001181\n",
      "Epoch 5600, Loss: 0.001164\n",
      "Epoch 5700, Loss: 0.001148\n",
      "Epoch 5800, Loss: 0.001133\n",
      "Epoch 5900, Loss: 0.001118\n",
      "Epoch 6000, Loss: 0.001104\n",
      "Epoch 6100, Loss: 0.001090\n",
      "Epoch 6200, Loss: 0.001076\n",
      "Epoch 6300, Loss: 0.001063\n",
      "Epoch 6400, Loss: 0.001050\n",
      "Epoch 6500, Loss: 0.001038\n",
      "Epoch 6600, Loss: 0.001026\n",
      "Epoch 6700, Loss: 0.001014\n",
      "Epoch 6800, Loss: 0.001002\n",
      "Epoch 6900, Loss: 0.000991\n",
      "Epoch 7000, Loss: 0.000980\n",
      "Epoch 7100, Loss: 0.000970\n",
      "Epoch 7200, Loss: 0.000960\n",
      "Epoch 7300, Loss: 0.000950\n",
      "Epoch 7400, Loss: 0.000940\n",
      "Epoch 7500, Loss: 0.000930\n",
      "Epoch 7600, Loss: 0.000921\n",
      "Epoch 7700, Loss: 0.000912\n",
      "Epoch 7800, Loss: 0.000903\n",
      "Epoch 7900, Loss: 0.000894\n",
      "Epoch 8000, Loss: 0.000886\n",
      "Epoch 8100, Loss: 0.000878\n",
      "Epoch 8200, Loss: 0.000870\n",
      "Epoch 8300, Loss: 0.000862\n",
      "Epoch 8400, Loss: 0.000854\n",
      "Epoch 8500, Loss: 0.000846\n",
      "Epoch 8600, Loss: 0.000839\n",
      "Epoch 8700, Loss: 0.000832\n",
      "Epoch 8800, Loss: 0.000825\n",
      "Epoch 8900, Loss: 0.000818\n",
      "Epoch 9000, Loss: 0.000811\n",
      "Epoch 9100, Loss: 0.000804\n",
      "Epoch 9200, Loss: 0.000798\n",
      "Epoch 9300, Loss: 0.000791\n",
      "Epoch 9400, Loss: 0.000785\n",
      "Epoch 9500, Loss: 0.000779\n",
      "Epoch 9600, Loss: 0.000773\n",
      "Epoch 9700, Loss: 0.000767\n",
      "Epoch 9800, Loss: 0.000761\n",
      "Epoch 9900, Loss: 0.000755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.0007496277611177607)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, layers: List[Layer]):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, X: np.ndarray):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backwards(self, X, y_true, learning_rate):\n",
    "        \n",
    "        y_pred = self.forward(X)\n",
    "        \n",
    "        output_delta = (y_pred - y_true) * da_dz(self.layers[-1].z)\n",
    "        \n",
    "        current_delta = output_delta\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            current_delta = layer.backward(current_delta, learning_rate)\n",
    "                    \n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        loss = 0\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            loss = np.mean((y_pred - y) ** 2)\n",
    "            \n",
    "            self.backwards(X, y, learning_rate)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "                \n",
    "        return loss\n",
    "                   \n",
    "            \n",
    "layer1 = Layer(input_size=2, neuron_count=2) \n",
    "layer2 = Layer(input_size=2, neuron_count=1)   \n",
    "\n",
    "layers = [layer1, layer2]\n",
    "network = Network(layers)\n",
    "\n",
    "network.train(X, y, 10000, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
