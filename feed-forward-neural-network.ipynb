{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "79894cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "87e64a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 0],\n",
    "              [0, 1], \n",
    "              [1, 0],\n",
    "              [1, 1]])\n",
    "\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1], \n",
    "              [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "de3eb851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "320c2df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "983c18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dC_da(a, y_true):\n",
    "    return 2*(a- y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "10ea45e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def da_dz(z):\n",
    "    return sigmoid(z) * (1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ca0ba7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_normal(fan_in, fan_out):\n",
    "    std = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "    return np.random.normal(0, std, (fan_out, fan_in)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6e0feaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size: int, neuron_count: int):\n",
    "        self.W = xavier_normal(input_size, neuron_count)\n",
    "        self.b = np.zeros((1, neuron_count))\n",
    "        \n",
    "        self.z = None\n",
    "        self.inputs = None\n",
    "        self.activations = None\n",
    "        \n",
    "    def forward(self, X: np.ndarray):   \n",
    "        self.inputs = X\n",
    "        self.z = X @ self.W + self.b\n",
    "        a = sigmoid(self.z)\n",
    "        self.activations = a\n",
    "        return a\n",
    "    \n",
    "    def backward(self, delta, learning_rate):\n",
    "        dz = np.array([])\n",
    "        \n",
    "\n",
    "        dz = delta * da_dz(self.z)\n",
    "        \n",
    "        dW = self.inputs.T @ dz\n",
    "        db = np.sum(dz, axis=0, keepdims=True)\n",
    "        \n",
    "        delta_prev = dz @ self.W.T\n",
    "        \n",
    "        self.W -= learning_rate * dW\n",
    "        self.b -= learning_rate * db\n",
    "        \n",
    "        return delta_prev\n",
    "    \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c2049768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5       , 0.5       , 0.5       ],\n",
       "       [0.52263993, 0.52716085, 0.62559217],\n",
       "       [0.29069861, 0.47483811, 0.4402997 ],\n",
       "       [0.30973215, 0.50200444, 0.56792916]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = Layer(input_size=2, neuron_count=3)\n",
    "layer.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac5a70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.262553\n",
      "Epoch 100, Loss: 0.250342\n",
      "Epoch 200, Loss: 0.248232\n",
      "Epoch 300, Loss: 0.247665\n",
      "Epoch 400, Loss: 0.247226\n",
      "Epoch 500, Loss: 0.246756\n",
      "Epoch 600, Loss: 0.246232\n",
      "Epoch 700, Loss: 0.245648\n",
      "Epoch 800, Loss: 0.244995\n",
      "Epoch 900, Loss: 0.244271\n",
      "Epoch 1000, Loss: 0.243469\n",
      "Epoch 1100, Loss: 0.242587\n",
      "Epoch 1200, Loss: 0.241622\n",
      "Epoch 1300, Loss: 0.240572\n",
      "Epoch 1400, Loss: 0.239436\n",
      "Epoch 1500, Loss: 0.238210\n",
      "Epoch 1600, Loss: 0.236894\n",
      "Epoch 1700, Loss: 0.235483\n",
      "Epoch 1800, Loss: 0.233976\n",
      "Epoch 1900, Loss: 0.232369\n",
      "Epoch 2000, Loss: 0.230660\n",
      "Epoch 2100, Loss: 0.228847\n",
      "Epoch 2200, Loss: 0.226928\n",
      "Epoch 2300, Loss: 0.224905\n",
      "Epoch 2400, Loss: 0.222780\n",
      "Epoch 2500, Loss: 0.220557\n",
      "Epoch 2600, Loss: 0.218243\n",
      "Epoch 2700, Loss: 0.215846\n",
      "Epoch 2800, Loss: 0.213378\n",
      "Epoch 2900, Loss: 0.210853\n",
      "Epoch 3000, Loss: 0.208285\n",
      "Epoch 3100, Loss: 0.205690\n",
      "Epoch 3200, Loss: 0.203083\n",
      "Epoch 3300, Loss: 0.200482\n",
      "Epoch 3400, Loss: 0.197901\n",
      "Epoch 3500, Loss: 0.195356\n",
      "Epoch 3600, Loss: 0.192857\n",
      "Epoch 3700, Loss: 0.190418\n",
      "Epoch 3800, Loss: 0.188046\n",
      "Epoch 3900, Loss: 0.185749\n",
      "Epoch 4000, Loss: 0.183532\n",
      "Epoch 4100, Loss: 0.181399\n",
      "Epoch 4200, Loss: 0.179353\n",
      "Epoch 4300, Loss: 0.177394\n",
      "Epoch 4400, Loss: 0.175521\n",
      "Epoch 4500, Loss: 0.173734\n",
      "Epoch 4600, Loss: 0.172032\n",
      "Epoch 4700, Loss: 0.170411\n",
      "Epoch 4800, Loss: 0.168868\n",
      "Epoch 4900, Loss: 0.167401\n",
      "Epoch 5000, Loss: 0.166007\n",
      "Epoch 5100, Loss: 0.164682\n",
      "Epoch 5200, Loss: 0.163423\n",
      "Epoch 5300, Loss: 0.162226\n",
      "Epoch 5400, Loss: 0.161088\n",
      "Epoch 5500, Loss: 0.160006\n",
      "Epoch 5600, Loss: 0.158977\n",
      "Epoch 5700, Loss: 0.157997\n",
      "Epoch 5800, Loss: 0.157065\n",
      "Epoch 5900, Loss: 0.156177\n",
      "Epoch 6000, Loss: 0.155330\n",
      "Epoch 6100, Loss: 0.154523\n",
      "Epoch 6200, Loss: 0.153753\n",
      "Epoch 6300, Loss: 0.153018\n",
      "Epoch 6400, Loss: 0.152316\n",
      "Epoch 6500, Loss: 0.151645\n",
      "Epoch 6600, Loss: 0.151003\n",
      "Epoch 6700, Loss: 0.150389\n",
      "Epoch 6800, Loss: 0.149801\n",
      "Epoch 6900, Loss: 0.149237\n",
      "Epoch 7000, Loss: 0.148697\n",
      "Epoch 7100, Loss: 0.148179\n",
      "Epoch 7200, Loss: 0.147681\n",
      "Epoch 7300, Loss: 0.147203\n",
      "Epoch 7400, Loss: 0.146744\n",
      "Epoch 7500, Loss: 0.146302\n",
      "Epoch 7600, Loss: 0.145878\n",
      "Epoch 7700, Loss: 0.145468\n",
      "Epoch 7800, Loss: 0.145075\n",
      "Epoch 7900, Loss: 0.144695\n",
      "Epoch 8000, Loss: 0.144329\n",
      "Epoch 8100, Loss: 0.143976\n",
      "Epoch 8200, Loss: 0.143635\n",
      "Epoch 8300, Loss: 0.143305\n",
      "Epoch 8400, Loss: 0.142987\n",
      "Epoch 8500, Loss: 0.142680\n",
      "Epoch 8600, Loss: 0.142382\n",
      "Epoch 8700, Loss: 0.142095\n",
      "Epoch 8800, Loss: 0.141816\n",
      "Epoch 8900, Loss: 0.141547\n",
      "Epoch 9000, Loss: 0.141285\n",
      "Epoch 9100, Loss: 0.141032\n",
      "Epoch 9200, Loss: 0.140786\n",
      "Epoch 9300, Loss: 0.140548\n",
      "Epoch 9400, Loss: 0.140317\n",
      "Epoch 9500, Loss: 0.140092\n",
      "Epoch 9600, Loss: 0.139874\n",
      "Epoch 9700, Loss: 0.139663\n",
      "Epoch 9800, Loss: 0.139457\n",
      "Epoch 9900, Loss: 0.139257\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "np.float64(0.1390640570850421)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "class Network:\n",
    "    def __init__(self, layers: List[Layer]):\n",
    "        self.layers = layers\n",
    "    \n",
    "    def forward(self, X: np.ndarray):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backwards(self, X, y_true, learning_rate):\n",
    "        \n",
    "        y_pred = self.forward(X)\n",
    "        \n",
    "        output_delta = (y_pred - y_true) * da_dz(self.layers[-1].z)\n",
    "        \n",
    "        current_delta = output_delta\n",
    "        \n",
    "        for layer in reversed(self.layers):\n",
    "            current_delta = layer.backward(current_delta, learning_rate)\n",
    "                    \n",
    "    def train(self, X, y, epochs, learning_rate):\n",
    "        loss = 0\n",
    "        for epoch in range(epochs):\n",
    "            y_pred = self.forward(X)\n",
    "            \n",
    "            loss = MSE(y, y_pred)\n",
    "            \n",
    "            self.backwards(X, y, learning_rate)\n",
    "            \n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.6f}\")\n",
    "                \n",
    "        return loss\n",
    "                   \n",
    "            \n",
    "layer1 = Layer(input_size=2, neuron_count=2)\n",
    "layer2 = Layer(input_size=2, neuron_count=2) \n",
    "layer3 = Layer(input_size=2, neuron_count=1)\n",
    "\n",
    "layers = [layer1, layer2]\n",
    "network = Network(layers)\n",
    "\n",
    "network.train(X, y, 20000, 0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
